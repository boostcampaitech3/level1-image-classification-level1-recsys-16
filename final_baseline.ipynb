{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "705315f7-14f4-49bb-96b8-d6822404f7ed",
   "metadata": {},
   "source": [
    "# 일단 이 파일을 본인의 디렉토리로 옮긴 후에 실행바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "351b06b7-aeb4-4c70-9c13-4d3d41e90d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import timm\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import math\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from adamp import AdamP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f850e517-a5c6-4b24-816b-27c840c89f75",
   "metadata": {},
   "source": [
    "# 이 부분은 정말로 주의가 필요합니다\n",
    "### 해당 코드는 정말로 딱 한번만 실행하세요. 2번도 안됩니다.. 딱 한번이요 한번 실행하고 나면 주석처리하거나 삭제해주세요~\n",
    "### 왜냐하면, 실제 이미지 파일에서 마스크 이상하게 쓴 사람과 안 쓴 사람이 서로 바뀌어져있는데,\n",
    "### 잘못된 이미지 파일이기 때문에 바꾸는 부분입니다.\n",
    "### 한번 실행하고 또 실행하면 또 바뀌니,, 다시 이상한 데이터가 되니깐 꼭 한번만 실행해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a0f245-6979-4bac-9e68-7f71b432782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir= \"/opt/ml/input/data/train/images\"\n",
    "\n",
    "# wrong_folders = [\"000020_female_Asian_50\", \"004418_male_Asian_20\", \"005227_male_Asian_22\"]\n",
    "\n",
    "# file_names = [\"incorrect_mask.jpg\", \"normal.jpg\", \"temp.jpg\"]\n",
    "\n",
    "# for folder in wrong_folders:\n",
    "\n",
    "#     image_dir = os.path.join(data_dir, folder)\n",
    "\n",
    "#     incorrect_file = os.path.join(image_dir, file_names[0])\n",
    "\n",
    "#     normal_file = os.path.join(image_dir, file_names[1])\n",
    "\n",
    "#     temp = os.path.join(image_dir, file_names[2])\n",
    "\n",
    "#     os.rename(incorrect_file, temp)  \n",
    "\n",
    "#     os.rename(normal_file, incorrect_file)\n",
    "\n",
    "#     os.rename(temp, normal_file)\n",
    "\n",
    "#     print(\"Changed File Names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98802e1f-d8b0-4649-8596-db59bb2c4791",
   "metadata": {},
   "source": [
    "## 이 부분은 사용할 데이터 처리하는 부분입니다.\n",
    "## 한번 돌리는데 시간이 조금 걸리는 편이니, 아래에 실행을 해서 데이터프레임이 만들어진다면,\n",
    "## 그냥 불러서 사용하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d41a98af-7adb-422a-aed6-a1348a65dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/opt/ml/input/data/train/images'\n",
    "test_dir = '/opt/ml/input/data/eval/'\n",
    "\n",
    "train_df = pd.read_csv(\"/opt/ml/input/data/train/train.csv\")\n",
    "\n",
    "train_df.loc[train_df['id']=='001498-1','gender'] = 'female'\n",
    "train_df.loc[train_df['id']=='004432','gender'] = 'female'\n",
    "train_df.loc[train_df['id']=='000010','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='000357','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='000664','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='000667','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='000725','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='000736','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='000767','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='000817','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='003780','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='003798','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='004281','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='006359','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='006360','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='006361','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='006362','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='006363','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='006364','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='006504','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='001009','age'] = 29\n",
    "train_df.loc[train_df['id']=='001064','age'] = 29\n",
    "train_df.loc[train_df['id']=='001637','age'] = 29\n",
    "train_df.loc[train_df['id']=='001666','age'] = 29\n",
    "train_df.loc[train_df['id']=='001852','age'] = 29\n",
    "train_df.loc[train_df['id']=='004348','age'] = 60\n",
    "\n",
    "def age_group(x):\n",
    "    if x < 30:\n",
    "        return 0\n",
    "    elif x < 58:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "def gender_group(x):\n",
    "    if x == 'male':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "df = pd.DataFrame(None, columns = ['gender', 'age','maskOX','maskGB','label','path'])\n",
    "\n",
    "for index, line in enumerate(train_df.iloc):\n",
    "    for file in list(os.listdir(os.path.join(train_dir, line['path']))):\n",
    "        if file[0] == '.':\n",
    "            continue\n",
    "        if file.split('.')[0] == 'normal':\n",
    "            mask = 2\n",
    "        elif file.split('.')[0] == 'incorrect_mask':\n",
    "            mask = 1\n",
    "        else:\n",
    "            mask = 0\n",
    "        gender = 0 if line['gender'] == 'male' else 1\n",
    "        data = {\n",
    "            'gender' : gender_group(line['gender']),\n",
    "            'age' : age_group(line['age']),\n",
    "            'maskOX' : 0 if mask == 2 else 1, # 마스크 안쓰면 0, 쓰면 1\n",
    "            'maskGB' : None if mask == 2 else (0 if mask == 1 else 1), # 마스크 안쓰면 none, 마스크 비정상이면 0, 마스크 정상이면 1\n",
    "            'path': os.path.join(train_dir, line['path'], file),\n",
    "            'label': mask * 6 + gender * 3 + age_group(line['age'])\n",
    "        }\n",
    "        df = df.append(data, ignore_index=True)\n",
    "# 마스크 5:1:1\n",
    "df.to_csv('/opt/ml/code/total.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(None, columns = ['gender', 'age','maskOX','maskGB','label','path'])\n",
    "\n",
    "for index, line in enumerate(train_df.iloc):\n",
    "    for file in list(os.listdir(os.path.join(train_dir, line['path']))):\n",
    "        if file[0] == '.':\n",
    "            continue\n",
    "        if file.split('.')[0] == 'normal':\n",
    "            mask = 2\n",
    "        elif file.split('.')[0] == 'incorrect_mask':\n",
    "            mask = 1\n",
    "        elif file.split('.')[0] == 'mask1':\n",
    "            mask = 0\n",
    "        else:\n",
    "            continue\n",
    "        gender = 0 if line['gender'] == 'male' else 1\n",
    "        data = {\n",
    "            'gender' : gender_group(line['gender']),\n",
    "            'age' : age_group(line['age']),\n",
    "            'maskOX' : 0 if mask == 2 else 1, # 마스크 안쓰면 0, 쓰면 1\n",
    "            'maskGB' : None if mask == 2 else (0 if mask == 1 else 1), # 마스크 안쓰면 none, 마스크 비정상이면 0, 마스크 정상이면 1\n",
    "            'path': os.path.join(train_dir, line['path'], file),\n",
    "            'label': mask * 6 + gender * 3 + age_group(line['age'])\n",
    "        }\n",
    "        df = df.append(data, ignore_index=True)\n",
    "# 마스크 1:1:1\n",
    "df.to_csv('/opt/ml/code/total_111.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee2ba37-bd0e-4261-8d7f-f695de2b9a0d",
   "metadata": {},
   "source": [
    "# 시드 및 모델의 기본적인 파라미터 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b38516b6-f427-4f58-8a7f-7169bd3cd9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed) #torch를 거치는 모든 난수들의 생성순서를 고정한다\n",
    "    torch.cuda.manual_seed(seed) #cuda를 사용하는 메소드들의 난수시드는 따로 고정해줘야한다 \n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True #딥러닝에 특화된 CuDNN의 난수시드도 고정 \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed) #numpy를 사용할 경우 고정\n",
    "    random.seed(seed) #파이썬 자체 모듈 random 모듈의 시드 고정\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e4f8f9-b4b6-4e5c-9742-c8af32bd56eb",
   "metadata": {},
   "source": [
    "# GPU 확인 및 device에 사용할 gpu 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "740102ce-4c29-4234-9cb5-a608c807e346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 1.6.0\n",
      "GPU 사용 가능 여부: True\n"
     ]
    }
   ],
   "source": [
    "print('pytorch version: {}'.format(torch.__version__))\n",
    "print('GPU 사용 가능 여부: {}'.format(torch.cuda.is_available()))\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"   # GPU 사용 가능 여부에 따라 device 정보 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce02e616-445b-4b47-94c5-352fbb6bafac",
   "metadata": {},
   "source": [
    "# 모델 돌릴 조건 나누는 곳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11663e4f-54f0-40ae-9f0c-e97993e9e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.read_csv(\"/opt/ml/code/total.csv\")\n",
    "\n",
    "# 1대1대1 비율로 돌리고 싶다면 아래 코드를 쓰세요\n",
    "# total = pd.read_csv(\"/opt/ml/code/total111.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f0b704a-1297-4eb7-b285-122bb4193af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_df_condition(df, num):\n",
    "    # 전체 데이터 활용\n",
    "    if num == 0:\n",
    "        return df\n",
    "    # 마스크 쓴 사람\n",
    "    elif num == 1:\n",
    "        temp_df = df.loc[df['maskOX']==1]\n",
    "        return temp_df\n",
    "    # 마스크 안쓴 사람\n",
    "    elif num == 2:\n",
    "        temp_df = df.loc[df['maskOX']==0]\n",
    "        return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "592fc5d1-b033-42ba-93cc-dbf1fe2af2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(target):\n",
    "    if target == \"label\":\n",
    "        return 18\n",
    "    elif target == \"gender\":\n",
    "        return 2\n",
    "    elif target == \"age\":\n",
    "        return 3\n",
    "    elif target == \"maskOX\":\n",
    "        return 2\n",
    "    elif target == \"maskGB\":\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce223c5f-6570-42fd-9d6c-d7dcd17e8df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래와 같은 조건이면, 전체 데이터 중에서 마스크 쓴 사람과 안 쓴사람을 목표로 한다\n",
    "df = change_df_condition(total, 0)\n",
    "target =\"label\"\n",
    "CLASS_NUM = output(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0963f5e-cf28-4337-9191-589af5a73f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train_test_split(df, test_size=0.2,\n",
    "                                shuffle=True, stratify=df[target],\n",
    "                                random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4512f507-29a6-4168-93aa-d0b26bf7b775",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53e8a0c6-e1e2-4131-a707-b2e71595efca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset): # 각 목표별 데이터셋 만들기\n",
    "    def __init__(self, df, transform, target):\n",
    "        self.df = df\n",
    "        self.img_paths = self.df['path'].tolist()\n",
    "        self.labels = self.df[target].tolist()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, torch.tensor(self.labels[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed7e29a-04ec-4271-bd44-aad3f472ce75",
   "metadata": {},
   "source": [
    "## transform 정의 -> 일단은 고정하고 추후에 바꿔보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c97ba2bd-34f6-48a3-8d74-a77dad96e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.CenterCrop(384),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.CenterCrop(384),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ec122ce-6481-470e-9176-be4c00dfc655",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = CustomDataset(train, transform['train'], target)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "                            train_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True\n",
    "                             )\n",
    "\n",
    "valid_dataset = CustomDataset(valid, transform['test'], target)\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "                            valid_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b8425-ce47-4903-a076-510195ebd373",
   "metadata": {},
   "source": [
    "# 모델생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a73f662f-d741-4b07-9501-65e42cbdb418",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.modules.loss._WeightedLoss):\n",
    "    def __init__(self, weight=None, gamma=2,reduction='mean'):\n",
    "        super(FocalLoss, self).__init__(weight,reduction=reduction)\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n",
    "\n",
    "    def forward(self, input, target):\n",
    "\n",
    "        ce_loss = F.cross_entropy(input, target,reduction=self.reduction,weight=self.weight)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecfdb85a-735e-4996-99a4-bdb4f53adf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b5\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNet.from_pretrained('efficientnet-b5', num_classes=18)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamP(model.parameters(),lr=5e-4,weight_decay=5e-4)\n",
    "criterion = FocalLoss()\n",
    "\n",
    "# 여기서 각자 저장할 모델 지정해주세요\n",
    "# 모델 저장해야지 중간에 종료해도 그 결과 남아서 써먹을수있어요~ 중간에 멈췄다가 다시 로드해서 재학습도 쌉가능\n",
    "saved_dir = '/opt/ml/level1-image-classification-level1-recsys-16/junghkim/model'\n",
    "val_every = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c62898-5b5b-4074-a7a3-1edd5c35792a",
   "metadata": {},
   "source": [
    "# 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86739f68-5420-4f59-b1ac-a61f18ecb5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_eval(model,data_iter,device):\n",
    "    with torch.no_grad():\n",
    "        n_total,n_correct = 0,0\n",
    "        model.eval() # evaluate (affects DropOut and BN)\n",
    "        for batch_in,batch_out in data_iter:\n",
    "            y_trgt = batch_out.to(device)\n",
    "            model_pred = model(batch_in.to(device))\n",
    "            _,y_pred = torch.max(model_pred.data,1)\n",
    "            n_correct += (y_pred==y_trgt).sum().item()\n",
    "            n_total += batch_in.size(0)\n",
    "        val_accr = (n_correct/n_total)\n",
    "        model.train() # back to train mode \n",
    "    return val_accr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4552e794-f7e8-46fa-a65a-71611504020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, model, data_loader, criterion, optimizer, saved_dir, val_every, device):\n",
    "    model.train()\n",
    "    print('Start training..')\n",
    "    total_start_time = timeit.default_timer()\n",
    "    best_loss = 9999999\n",
    "    best_test_accuracy = 0\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        epoch_f1 = 0\n",
    "        running_acc = 0\n",
    "        epoch_loss = 0\n",
    "        print('Epoch start..')\n",
    "        epoch_start_time = timeit.default_timer()\n",
    "        for i, (imgs, labels) in enumerate(data_loader):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            ## 코드 시작 ##\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()         \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, argmax = torch.max(outputs, 1)\n",
    "            accuracy = (labels == argmax).float().mean()\n",
    "            \n",
    "            f1 = f1_score(labels.detach().cpu().numpy(), argmax.detach().cpu().numpy(), average='weighted')\n",
    "            epoch_f1 += f1\n",
    "            running_acc += accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            if (i+1) % 3 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.10f}, Accuracy: {:.2f}%, F1_Score: {:.2f}'.format(\n",
    "                    epoch+1, num_epochs, i+1, len(data_loader), loss.item(), accuracy.item() * 100, f1))\n",
    "        print(\"------------Epoch Finish------------\")\n",
    "        avrg_epoch_loss = epoch_loss/(i+1)\n",
    "        train_accr = func_eval(model,data_loader,device)\n",
    "        print('Epoch [{}/{}], Avrg Accuracy: {:.2f}%, Avrg Loss: {:.10f}, Train Accuracy: {:.2f}%, F1_Score: {:.2f}'.format(\n",
    "                    epoch+1, num_epochs, running_acc.item()/(i+1) * 100, avrg_epoch_loss,train_accr * 100, epoch_f1/(i+1)))\n",
    "        if (epoch + 1) % val_every == 0:\n",
    "            avrg_val_loss = validation(epoch + 1, model, valid_dataloader, criterion, device)\n",
    "            print(\"avrg val loss : {:.10f}\".format(avrg_val_loss))\n",
    "            if avrg_val_loss < best_loss:\n",
    "                print('Best performance at epoch: {}'.format(epoch + 1))\n",
    "                print('Save model in', saved_dir)\n",
    "                best_loss = avrg_val_loss\n",
    "                save_model(model, saved_dir,epoch)\n",
    "        epoch_end_time = timeit.default_timer()\n",
    "        print(\"Epoch end..\")\n",
    "        print(f\"epoch time : {epoch_end_time-epoch_start_time}\")\n",
    "        epoch_acc = running_acc / (i+1)\n",
    "        \n",
    "        if best_test_accuracy < epoch_acc:\n",
    "            best_test_accuracy = epoch_acc\n",
    "            save_model(model, saved_dir,epoch)\n",
    "            early_stop_point = 0\n",
    "        else:\n",
    "            early_stop_point += 1\n",
    "        if early_stop_point == 3:\n",
    "            print('early_stopped')\n",
    "            break\n",
    "    print('End training..')\n",
    "    total_end_time = timeit.default_timer()\n",
    "    print(f\"total time : {total_end_time-total_start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "174bcf2f-2cfd-4443-b841-3b54326c8937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch, model, data_loader, criterion, device):\n",
    "    print('Start validation #{}'.format(epoch) )\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        total_loss = 0\n",
    "        cnt = 0\n",
    "        epoch_f1 = 0\n",
    "        for i, (imgs, labels) in enumerate(data_loader):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            ## 코드 시작 ##\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            ## 코드 종료 ##\n",
    "            total += imgs.size(0)\n",
    "            _, argmax = torch.max(outputs, 1)\n",
    "            correct += (labels == argmax).sum().item()\n",
    "            total_loss += loss\n",
    "            cnt += 1\n",
    "            epoch_f1 += f1_score(labels.detach().cpu().numpy(), argmax.detach().cpu().numpy(), average='weighted')\n",
    "        avrg_loss = total_loss / cnt\n",
    "        avrg_f1 = epoch_f1 / cnt\n",
    "        print('Validation #{}  Accuracy: {:.2f}% F1_Score: {:.2f} Average Loss: {:.10f}'.format(epoch, correct / total * 100,avrg_f1 ,avrg_loss))\n",
    "    model.train()\n",
    "    return avrg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c07829-3c06-4ab3-92f4-18c3f968eb0c",
   "metadata": {},
   "source": [
    "# 모델 저장 하는 함수\n",
    "### 여기서 각자 저장할 모델이름을 변경해주세요~ 조건이나 날짜 시간 등을 추가하면 구분하기 쉽겠죠?\n",
    "### 매번 새로 돌릴때마다 꼭 변경해서 기록해야지 나중에 확인하기 편합니다\n",
    "### 여기 수정할 때, 위에 모델들의 조건을 정리해서 노션에 올리면 더 좋겠죠??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72e533c2-498e-451c-bcb2-c28711c02e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, saved_dir,epoch):\n",
    "    os.makedirs(saved_dir, exist_ok=True)\n",
    "    file_name=f'effi_b5_epoch{epoch}.pt'\n",
    "    check_point = {\n",
    "        'net': model.state_dict()\n",
    "    }\n",
    "    output_path = os.path.join(saved_dir, file_name)\n",
    "    torch.save(check_point,output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4ab239-db3b-467f-ad6d-4dba7f8d592b",
   "metadata": {},
   "source": [
    "### 학습 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfbe14f2-03ef-421f-bef2-9d7a02c5db8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training..\n",
      "Epoch start..\n",
      "Epoch [1/20], Step [3/473], Loss: 2.4543766975, Accuracy: 25.00%, F1_Score: 0.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-548b6af09363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-3ca7bf55acf1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs, model, data_loader, criterion, optimizer, saved_dir, val_every, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/adamp/adamp.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mwd_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                     \u001b[0mperturb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_projection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperturb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'delta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wd_ratio'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;31m# Weight decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/adamp/adamp.py\u001b[0m in \u001b[0;36m_projection\u001b[0;34m(self, p, grad, perturb, delta, wd_ratio, eps)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mcosine_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mcosine_sim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mview_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0mp_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mview_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mperturb\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mp_n\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mview_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_n\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mperturb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "train(num_epochs, model, train_dataloader, criterion, optimizer, saved_dir, val_every, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd49c163-a9af-4951-815e-1eb81f6f7422",
   "metadata": {},
   "source": [
    "# 모델 불러오는 곳\n",
    "### 어제 돌리다가 일어나서 돌리던 모델 다시 돌리고 싶으면. 아래에서 모델 불러와서 쓰면 됩니다.\n",
    "### 대신 완전히 껐다가 킨거면, 위에 train 함수 전까지 다 실행시키고 아래 코드 실행하고 다시 train 학습 하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23649c9f-dc9d-4799-b28a-1e743ed32b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = '/opt/ml/level1-image-classification-level1-recsys-16/junghkim/model/effi_b5_epoch2.pt'\n",
    "checkpoint = torch.load(model_path,map_location=device)\n",
    "state_dict = checkpoint['net']\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc340cc-60cc-43d1-b88b-4459a1ebc652",
   "metadata": {},
   "source": [
    "# 틀린 부분 확인하는 곳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "905ca92f-f28a-4b20-8435-dee1ddcac0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_eval(raw_data, dataloader, model, device):\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, (X,y) in enumerate(dataloader):\n",
    "            model_pred = model.forward(X.to(device))\n",
    "            _, y_pred = torch.max(model_pred, 1)\n",
    "            \n",
    "            result.append([valid.iloc[i]['path'], y_pred.cpu().numpy()[0], y.cpu().numpy()[0]])\n",
    "    result = pd.DataFrame(result, columns=['path', 'pred', 'target'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac6eed2f-09ab-4b1d-9af0-714641f2f1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_testing_dataloader = DataLoader(valid_dataset, shuffle = False)\n",
    "check_eval_df = check_eval(valid, valid_testing_dataloader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2debc17-0880-4d98-8524-1d4830eaa40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_df = check_eval_df[check_eval_df['pred'] != check_eval_df['target']]\n",
    "wrong_df = wrong_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e9e16c9-7448-47d5-9ad0-10f7ae01fbde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 3600x7200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(wrong_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b95e03-2c3c-4763-98fb-0bde41e50c60",
   "metadata": {},
   "source": [
    "# 아래는 실제 제출하기 위한 공간입니다\n",
    "## class(18)가 아니면 안쓰면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa13bb8e-8783-4b73-a426-979ce5eec38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a4971d94-cf69-45c6-8e54-7fc92c04492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '/opt/ml/input/data/eval/'\n",
    "\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "\n",
    "test_data = TestDataset(image_paths, transform=transform['test'])\n",
    "test_loader = DataLoader(test_data, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8ebf2b12-1460-41ad-8069-bbeca818bb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([13], device='cuda:0')\n",
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "model.eval()\n",
    "i = 0\n",
    "for images in test_loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        _, predict = torch.max(pred, 1)\n",
    "        if i < 3:\n",
    "            i+=1\n",
    "            print(predict)\n",
    "        all_predictions.extend(predict.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submit_dir = '/opt/ml/level1-image-classification-level1-recsys-16/junghkim/submit'\n",
    "submission.to_csv(os.path.join(submit_dir, 'effi_2.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eb2ee1-2ffc-4aa7-b807-99cc2efa1b31",
   "metadata": {},
   "source": [
    "## 여기서 i를 바꿔가면서 확인해보자\n",
    "## 확인하면서 깃허브에 있는 info.csv를 채워보는건 어떨까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1081ffc9-2533-4b4e-bc28-0087e39b2c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 209\n",
    "a = submission['ans'][i] // 6\n",
    "b = submission['ans'][i] % 6\n",
    "c = submission['ans'][i] % 3\n",
    "print(submission['ans'][i])\n",
    "if a == 0:\n",
    "    print(\"마스크씀\")\n",
    "elif a == 1:\n",
    "    print(\"마스크 비정상\")\n",
    "else:\n",
    "    print(\"마스크 안씀\")\n",
    "if b < 3:\n",
    "    print(\"남자\")\n",
    "else:\n",
    "    print(\"여자\")\n",
    "if c == 0 :\n",
    "    print(\"젊음\")\n",
    "elif c == 1:\n",
    "    print(\"중년\")\n",
    "else:\n",
    "    print(\"노년\")\n",
    "temp_dir = os.path.join(image_dir, submission['ImageID'][i])\n",
    "print(temp_dir)\n",
    "Image.open(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5770cda-3fbe-4494-beb5-579442077583",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission2 = pd.read_csv(os.path.join('/opt/ml/level1-image-classification-level1-recsys-16/', 'info.csv'))\n",
    "f1_score(submission2.iloc[:200]['ans'], submission.iloc[:200]['ans'], average='macro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
