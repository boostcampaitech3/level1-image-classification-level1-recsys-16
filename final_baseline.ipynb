{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "705315f7-14f4-49bb-96b8-d6822404f7ed",
   "metadata": {},
   "source": [
    "# 일단 이 파일을 본인의 디렉토리로 옮긴 후에 실행바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "351b06b7-aeb4-4c70-9c13-4d3d41e90d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import timm\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f850e517-a5c6-4b24-816b-27c840c89f75",
   "metadata": {},
   "source": [
    "# 이 부분은 정말로 주의가 필요합니다\n",
    "### 해당 코드는 정말로 딱 한번만 실행하세요. 2번도 안됩니다.. 딱 한번이요 한번 실행하고 나면 주석처리하거나 삭제해주세요~\n",
    "### 왜냐하면, 실제 이미지 파일에서 마스크 이상하게 쓴 사람과 안 쓴 사람이 서로 바뀌어져있는데,\n",
    "### 잘못된 이미지 파일이기 때문에 바꾸는 부분입니다.\n",
    "### 한번 실행하고 또 실행하면 또 바뀌니,, 다시 이상한 데이터가 되니깐 꼭 한번만 실행해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76a0f245-6979-4bac-9e68-7f71b432782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir= \"/opt/ml/input/data/train/images\"\n",
    "\n",
    "# wrong_folders = [\"000020_female_Asian_50\", \"004418_male_Asian_20\", \"005227_male_Asian_22\"]\n",
    "\n",
    "# file_names = [\"incorrect_mask.jpg\", \"normal.jpg\", \"temp.jpg\"]\n",
    "\n",
    "# for folder in wrong_folders:\n",
    "\n",
    "#     image_dir = os.path.join(data_dir, folder)\n",
    "\n",
    "#     incorrect_file = os.path.join(image_dir, file_names[0])\n",
    "\n",
    "#     normal_file = os.path.join(image_dir, file_names[1])\n",
    "\n",
    "#     temp = os.path.join(image_dir, file_names[2])\n",
    "\n",
    "#     os.rename(incorrect_file, temp)  \n",
    "\n",
    "#     os.rename(normal_file, incorrect_file)\n",
    "\n",
    "#     os.rename(temp, normal_file)\n",
    "\n",
    "#     print(\"Changed File Names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98802e1f-d8b0-4649-8596-db59bb2c4791",
   "metadata": {},
   "source": [
    "## 이 부분은 사용할 데이터 처리하는 부분입니다.\n",
    "## 한번 돌리는데 시간이 조금 걸리는 편이니, 아래에 실행을 해서 데이터프레임이 만들어진다면,\n",
    "## 그냥 불러서 사용하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d41a98af-7adb-422a-aed6-a1348a65dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/opt/ml/input/data/train/images'\n",
    "test_dir = '/opt/ml/input/data/eval/'\n",
    "\n",
    "train_df = pd.read_csv(\"/opt/ml/input/data/train/train.csv\")\n",
    "\n",
    "train_df.loc[train_df['id']=='001498-1','gender'] = 'female'\n",
    "train_df.loc[train_df['id']=='004432','gender'] = 'female'\n",
    "train_df.loc[train_df['id']=='000010','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='000357','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='000664','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='000667','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='000725','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='000736','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='000767','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='000817','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='003780','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='003798','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='004281','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='006359','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='006360','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='006361','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='006362','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='006363','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='006364','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='006504','gender'] = 'male'\n",
    "train_df.loc[train_df['id']=='001009','age'] = 29\n",
    "train_df.loc[train_df['id']=='001064','age'] = 29\n",
    "train_df.loc[train_df['id']=='001637','age'] = 29\n",
    "train_df.loc[train_df['id']=='001666','age'] = 29\n",
    "train_df.loc[train_df['id']=='001852','age'] = 29\n",
    "train_df.loc[train_df['id']=='004348','age'] = 60\n",
    "\n",
    "def age_group(x):\n",
    "    if x < 30:\n",
    "        return 0\n",
    "    elif x < 60:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "def gender_group(x):\n",
    "    if x == 'male':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "df = pd.DataFrame(None, columns = ['gender', 'age','maskOX','maskGB','class','path'])\n",
    "\n",
    "for index, line in enumerate(train_df.iloc):\n",
    "    for file in list(os.listdir(os.path.join(train_dir, line['path']))):\n",
    "        if file[0] == '.':\n",
    "            continue\n",
    "        if file.split('.')[0] == 'normal':\n",
    "            mask = 2\n",
    "        elif file.split('.')[0] == 'incorrect_mask':\n",
    "            mask = 1\n",
    "        else:\n",
    "            mask = 0\n",
    "        gender = 0 if line['gender'] == 'male' else 1\n",
    "        data = {\n",
    "            'gender' : gender_group(line['gender']),\n",
    "            'age' : age_group(line['age']),\n",
    "            'maskOX' : 0 if mask == 2 else 1, # 마스크 안쓰면 0, 쓰면 1\n",
    "            'maskGB' : None if mask == 2 else (0 if mask == 1 else 1), # 마스크 안쓰면 none, 마스크 비정상이면 0, 마스크 정상이면 1\n",
    "            'path': os.path.join(train_dir, line['path'], file),\n",
    "            'class': mask * 6 + gender * 3 + age_group(line['age'])\n",
    "        }\n",
    "        df = df.append(data, ignore_index=True)\n",
    "# 마스크 5:1:1\n",
    "df.to_csv('/opt/ml/code/total.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(None, columns = ['gender', 'age','maskOX','maskGB','class','path'])\n",
    "\n",
    "for index, line in enumerate(train_df.iloc):\n",
    "    for file in list(os.listdir(os.path.join(train_dir, line['path']))):\n",
    "        if file[0] == '.':\n",
    "            continue\n",
    "        if file.split('.')[0] == 'normal':\n",
    "            mask = 2\n",
    "        elif file.split('.')[0] == 'incorrect_mask':\n",
    "            mask = 1\n",
    "        elif file.split('.')[0] == 'mask1':\n",
    "            mask = 0\n",
    "        else:\n",
    "            continue\n",
    "        gender = 0 if line['gender'] == 'male' else 1\n",
    "        data = {\n",
    "            'gender' : gender_group(line['gender']),\n",
    "            'age' : age_group(line['age']),\n",
    "            'maskOX' : 0 if mask == 2 else 1, # 마스크 안쓰면 0, 쓰면 1\n",
    "            'maskGB' : None if mask == 2 else (0 if mask == 1 else 1), # 마스크 안쓰면 none, 마스크 비정상이면 0, 마스크 정상이면 1\n",
    "            'path': os.path.join(train_dir, line['path'], file),\n",
    "            'class': mask * 6 + gender * 3 + age_group(line['age'])\n",
    "        }\n",
    "        df = df.append(data, ignore_index=True)\n",
    "# 마스크 1:1:1\n",
    "df.to_csv('/opt/ml/code/total_111.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee2ba37-bd0e-4261-8d7f-f695de2b9a0d",
   "metadata": {},
   "source": [
    "# 시드 및 모델의 기본적인 파라미터 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b38516b6-f427-4f58-8a7f-7169bd3cd9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed) #torch를 거치는 모든 난수들의 생성순서를 고정한다\n",
    "    torch.cuda.manual_seed(seed) #cuda를 사용하는 메소드들의 난수시드는 따로 고정해줘야한다 \n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True #딥러닝에 특화된 CuDNN의 난수시드도 고정 \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed) #numpy를 사용할 경우 고정\n",
    "    random.seed(seed) #파이썬 자체 모듈 random 모듈의 시드 고정\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e4f8f9-b4b6-4e5c-9742-c8af32bd56eb",
   "metadata": {},
   "source": [
    "# GPU 확인 및 device에 사용할 gpu 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "740102ce-4c29-4234-9cb5-a608c807e346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 1.6.0\n",
      "GPU 사용 가능 여부: True\n"
     ]
    }
   ],
   "source": [
    "print('pytorch version: {}'.format(torch.__version__))\n",
    "print('GPU 사용 가능 여부: {}'.format(torch.cuda.is_available()))\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"   # GPU 사용 가능 여부에 따라 device 정보 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce02e616-445b-4b47-94c5-352fbb6bafac",
   "metadata": {},
   "source": [
    "# 모델 돌릴 조건 나누는 곳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11663e4f-54f0-40ae-9f0c-e97993e9e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.read_csv(\"/opt/ml/code/total.csv\")\n",
    "\n",
    "# 1대1대1 비율로 돌리고 싶다면 아래 코드를 쓰세요\n",
    "# total = pd.read_csv(\"/opt/ml/code/total111.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f0b704a-1297-4eb7-b285-122bb4193af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_df_condition(df, num):\n",
    "    # 전체 데이터 활용\n",
    "    if num == 0:\n",
    "        return df\n",
    "    # 마스크 쓴 사람\n",
    "    elif num == 1:\n",
    "        temp_df = df.loc[df['maskOX']==1]\n",
    "        return temp_df\n",
    "    # 마스크 안쓴 사람\n",
    "    elif num == 2:\n",
    "        temp_df = df.loc[df['maskOX']==0]\n",
    "        return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "592fc5d1-b033-42ba-93cc-dbf1fe2af2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(target):\n",
    "    if target == \"class\":\n",
    "        return 18\n",
    "    elif target == \"gender\":\n",
    "        return 2\n",
    "    elif target == \"age\":\n",
    "        return 3\n",
    "    elif target == \"maskOX\":\n",
    "        return 2\n",
    "    elif target == \"maskGB\":\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce223c5f-6570-42fd-9d6c-d7dcd17e8df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래와 같은 조건이면, 전체 데이터 중에서 마스크 쓴 사람과 안 쓴사람을 목표로 한다\n",
    "df = change_df_condition(total, 0)\n",
    "target =\"maskOX\"\n",
    "CLASS_NUM = output(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0963f5e-cf28-4337-9191-589af5a73f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train_test_split(df, test_size=0.2,\n",
    "                                shuffle=True, stratify=df[target],\n",
    "                                random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4512f507-29a6-4168-93aa-d0b26bf7b775",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53e8a0c6-e1e2-4131-a707-b2e71595efca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset): # 각 목표별 데이터셋 만들기\n",
    "    def __init__(self, df, transform, target):\n",
    "        self.df = df\n",
    "        self.img_paths = self.df['path'].tolist()\n",
    "        self.labels = self.df[target].tolist()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, torch.tensor(self.labels[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed7e29a-04ec-4271-bd44-aad3f472ce75",
   "metadata": {},
   "source": [
    "## transform 정의 -> 일단은 고정하고 추후에 바꿔보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c97ba2bd-34f6-48a3-8d74-a77dad96e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.CenterCrop(384),\n",
    "        transforms.RandomRotation(5),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.CenterCrop(384),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ec122ce-6481-470e-9176-be4c00dfc655",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = CustomDataset(train, transform['train'], target)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "                            train_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True\n",
    "                             )\n",
    "\n",
    "valid_dataset = CustomDataset(valid, transform['test'], target)\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "                            valid_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b8425-ce47-4903-a076-510195ebd373",
   "metadata": {},
   "source": [
    "# 모델생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a73f662f-d741-4b07-9501-65e42cbdb418",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.modules.loss._WeightedLoss):\n",
    "    def __init__(self, weight=None, gamma=2,reduction='mean'):\n",
    "        super(FocalLoss, self).__init__(weight,reduction=reduction)\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n",
    "\n",
    "    def forward(self, input, target):\n",
    "\n",
    "        ce_loss = F.cross_entropy(input, target,reduction=self.reduction,weight=self.weight)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3695bf-b6d7-4af9-a39e-49798c562f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet이랑 vit랑 알아서 구분해서 쓰세용~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecfdb85a-735e-4996-99a4-bdb4f53adf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('vit_large_patch32_384', pretrained=True)\n",
    "model.head = nn.Linear(model.head.in_features, CLASS_NUM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3adde70-322a-4a17-991f-a78602d86ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = torchvision.models.resnet18(pretrained=True)\n",
    "# model.fc = torch.nn.Linear(in_features=512, out_features=CLASS_NUM, bias=True)\n",
    "\n",
    "# torch.nn.init.xavier_uniform_(model.fc.weight)\n",
    "# stdv = 1. / math.sqrt(model.fc.weight.size(1))\n",
    "# model.fc.bias.data.uniform_(-stdv, stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7587d03-2326-4460-8a23-fc522aa2fbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "LEARNIG_RATE = 0.0001\n",
    "num_epochs = 20\n",
    "\n",
    "criterion = FocalLoss() # cross 에서 변경\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNIG_RATE)\n",
    "# 여기서 각자 저장할 모델 지정해주세요\n",
    "# 모델 저장해야지 중간에 종료해도 그 결과 남아서 써먹을수있어요~ 중간에 멈췄다가 다시 로드해서 재학습도 쌉가능\n",
    "saved_dir = '/opt/ml/level1-image-classification-level1-recsys-16/junghkim/model'\n",
    "val_every = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c62898-5b5b-4074-a7a3-1edd5c35792a",
   "metadata": {},
   "source": [
    "# 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86739f68-5420-4f59-b1ac-a61f18ecb5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_eval(model,data_iter,device):\n",
    "    with torch.no_grad():\n",
    "        n_total,n_correct = 0,0\n",
    "        model.eval() # evaluate (affects DropOut and BN)\n",
    "        for batch_in,batch_out in data_iter:\n",
    "            y_trgt = batch_out.to(device)\n",
    "            model_pred = model(batch_in.to(device))\n",
    "            _,y_pred = torch.max(model_pred.data,1)\n",
    "            n_correct += (y_pred==y_trgt).sum().item()\n",
    "            n_total += batch_in.size(0)\n",
    "        val_accr = (n_correct/n_total)\n",
    "        model.train() # back to train mode \n",
    "    return val_accr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4552e794-f7e8-46fa-a65a-71611504020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, model, data_loader, criterion, optimizer, saved_dir, val_every, device):\n",
    "    model.train()\n",
    "    print('Start training..')\n",
    "    total_start_time = timeit.default_timer()\n",
    "    best_loss = 9999999\n",
    "    best_test_accuracy = 0\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        epoch_f1 = 0\n",
    "        running_acc = 0\n",
    "        epoch_loss = 0\n",
    "        print('Epoch start..')\n",
    "        epoch_start_time = timeit.default_timer()\n",
    "        for i, (imgs, labels) in enumerate(data_loader):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            ## 코드 시작 ##\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()         \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, argmax = torch.max(outputs, 1)\n",
    "            accuracy = (labels == argmax).float().mean()\n",
    "            \n",
    "            f1 = f1_score(labels.detach().cpu().numpy(), argmax.detach().cpu().numpy(), average='weighted')\n",
    "            epoch_f1 += f1\n",
    "            running_acc += accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            if (i+1) % 3 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.10f}, Accuracy: {:.2f}%, F1_Score: {:.2f}'.format(\n",
    "                    epoch+1, num_epochs, i+1, len(data_loader), loss.item(), accuracy.item() * 100, f1))\n",
    "        print(\"------------Epoch Finish------------\")\n",
    "        avrg_epoch_loss = epoch_loss/(i+1)\n",
    "        train_accr = func_eval(model,data_loader,device)\n",
    "        print('Epoch [{}/{}], Avrg Accuracy: {:.2f}%, Avrg Loss: {:.10f}, Train Accuracy: {:.2f}%, F1_Score: {:.2f}'.format(\n",
    "                    epoch+1, num_epochs, running_acc.item()/(i+1) * 100, avrg_epoch_loss,train_accr * 100, epoch_f1/(i+1)))\n",
    "        if (epoch + 1) % val_every == 0:\n",
    "            avrg_val_loss = validation(epoch + 1, model, valid_dataloader, criterion, device)\n",
    "            print(\"avrg val loss : {:.10f}\".format(avrg_val_loss))\n",
    "            if avrg_val_loss < best_loss:\n",
    "                print('Best performance at epoch: {}'.format(epoch + 1))\n",
    "                print('Save model in', saved_dir)\n",
    "                best_loss = avrg_val_loss\n",
    "                save_model(model, saved_dir)\n",
    "        epoch_end_time = timeit.default_timer()\n",
    "        print(\"Epoch end..\")\n",
    "        print(f\"epoch time : {epoch_end_time-epoch_start_time}\")\n",
    "        epoch_acc = running_acc / (i+1)\n",
    "        \n",
    "        if best_test_accuracy < epoch_acc:\n",
    "            best_test_accuracy = epoch_acc\n",
    "            save_model(model, saved_dir)\n",
    "            early_stop_point = 0\n",
    "        else:\n",
    "            early_stop_point += 1\n",
    "        if early_stop_point == 3:\n",
    "            print('early_stopped')\n",
    "            break\n",
    "    print('End training..')\n",
    "    total_end_time = timeit.default_timer()\n",
    "    print(f\"total time : {total_end_time-total_start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "174bcf2f-2cfd-4443-b841-3b54326c8937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch, model, data_loader, criterion, device):\n",
    "    print('Start validation #{}'.format(epoch) )\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        total_loss = 0\n",
    "        cnt = 0\n",
    "        epoch_f1 = 0\n",
    "        for i, (imgs, labels) in enumerate(data_loader):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            ## 코드 시작 ##\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            ## 코드 종료 ##\n",
    "            total += imgs.size(0)\n",
    "            _, argmax = torch.max(outputs, 1)\n",
    "            correct += (labels == argmax).sum().item()\n",
    "            total_loss += loss\n",
    "            cnt += 1\n",
    "            epoch_f1 += f1_score(labels.detach().cpu().numpy(), argmax.detach().cpu().numpy(), average='weighted')\n",
    "        avrg_loss = total_loss / cnt\n",
    "        avrg_f1 = epoch_f1 / cnt\n",
    "        print('Validation #{}  Accuracy: {:.2f}% F1_Score: {:.2f} Average Loss: {:.10f}'.format(epoch, correct / total * 100,avrg_f1 ,avrg_loss))\n",
    "    model.train()\n",
    "    return avrg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c07829-3c06-4ab3-92f4-18c3f968eb0c",
   "metadata": {},
   "source": [
    "# 모델 저장 하는 함수\n",
    "### 여기서 각자 저장할 모델이름을 변경해주세요~ 조건이나 날짜 시간 등을 추가하면 구분하기 쉽겠죠?\n",
    "### 매번 새로 돌릴때마다 꼭 변경해서 기록해야지 나중에 확인하기 편합니다\n",
    "### 여기 수정할 때, 위에 모델들의 조건을 정리해서 노션에 올리면 더 좋겠죠??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72e533c2-498e-451c-bcb2-c28711c02e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, saved_dir, file_name='vit_focal_maskOX.pt'):\n",
    "    os.makedirs(saved_dir, exist_ok=True)\n",
    "    check_point = {\n",
    "        'net': model.state_dict()\n",
    "    }\n",
    "    output_path = os.path.join(saved_dir, file_name)\n",
    "    torch.save(check_point,output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4ab239-db3b-467f-ad6d-4dba7f8d592b",
   "metadata": {},
   "source": [
    "### 학습 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfbe14f2-03ef-421f-bef2-9d7a02c5db8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training..\n",
      "Epoch start..\n",
      "Epoch [1/20], Step [3/237], Loss: 0.0000006477, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [6/237], Loss: 0.0000004935, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [9/237], Loss: 0.0000002128, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [12/237], Loss: 0.0000003483, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [15/237], Loss: 0.0000028963, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [18/237], Loss: 0.0000000745, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [21/237], Loss: 0.0000000997, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [24/237], Loss: 0.0000002336, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [27/237], Loss: 0.0000020111, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [30/237], Loss: 0.0000003105, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [33/237], Loss: 0.0000005059, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [36/237], Loss: 0.0000002494, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [39/237], Loss: 0.0000001294, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [42/237], Loss: 0.0000005255, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [45/237], Loss: 0.0000002608, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [48/237], Loss: 0.0000002300, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [51/237], Loss: 0.0000005615, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [54/237], Loss: 0.0000006711, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [57/237], Loss: 0.0000000814, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [60/237], Loss: 0.0000002803, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [63/237], Loss: 0.0000004949, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [66/237], Loss: 0.0000001064, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [69/237], Loss: 0.0000002022, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [72/237], Loss: 0.0000004175, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [75/237], Loss: 0.0000005138, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [78/237], Loss: 0.0000002745, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [81/237], Loss: 0.0000001357, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [84/237], Loss: 0.0000000830, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [87/237], Loss: 0.0000000261, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [90/237], Loss: 0.0000010954, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [93/237], Loss: 0.0000001201, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [96/237], Loss: 0.0000005195, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [99/237], Loss: 0.0000001005, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [102/237], Loss: 0.0000006381, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [105/237], Loss: 0.0000003941, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [108/237], Loss: 0.0000005482, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [111/237], Loss: 0.0000000526, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [114/237], Loss: 0.0000001533, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [117/237], Loss: 0.0000004529, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [120/237], Loss: 0.0000002339, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [123/237], Loss: 0.0000003219, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [126/237], Loss: 0.0000001625, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [129/237], Loss: 0.0000003412, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [132/237], Loss: 0.0000001537, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [135/237], Loss: 0.0000000913, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [138/237], Loss: 0.0000001918, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [141/237], Loss: 0.0000002727, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [144/237], Loss: 0.0000001532, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [147/237], Loss: 0.0000003644, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [150/237], Loss: 0.0000006226, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [153/237], Loss: 0.0000001201, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [156/237], Loss: 0.0000000737, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [159/237], Loss: 0.0000000824, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [162/237], Loss: 0.0000000858, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [165/237], Loss: 0.0000001406, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [168/237], Loss: 0.0000002087, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [171/237], Loss: 0.0000008196, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [174/237], Loss: 0.0000009319, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [177/237], Loss: 0.0000000290, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [180/237], Loss: 0.0000006331, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [183/237], Loss: 0.0000002481, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [186/237], Loss: 0.0000000412, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [189/237], Loss: 0.0000000940, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [192/237], Loss: 0.0000001084, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [195/237], Loss: 0.0000000947, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [198/237], Loss: 0.0000009142, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [201/237], Loss: 0.0000004209, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [204/237], Loss: 0.0000000852, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [207/237], Loss: 0.0000001110, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [210/237], Loss: 0.0000001757, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [213/237], Loss: 0.0000002287, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [216/237], Loss: 0.0000003783, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [219/237], Loss: 0.0000000951, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [222/237], Loss: 0.0000005869, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [225/237], Loss: 0.0000001115, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [228/237], Loss: 0.0000000919, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [231/237], Loss: 0.0000001091, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [234/237], Loss: 0.0000001618, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [1/20], Step [237/237], Loss: 0.0000002229, Accuracy: 100.00%, F1_Score: 1.00\n",
      "------------Epoch Finish------------\n",
      "Epoch [1/20], Accuracy: 99.98%, F1_Score: 1.00\n",
      "Start validation #1\n",
      "Validation #1  Accuracy: 100.00% F1_Score: 1.00 Average Loss: 0.0000013314\n",
      "avrg loss : 0.0000013314\n",
      "Best performance at epoch: 1\n",
      "Save model in /opt/ml/level1-image-classification-level1-recsys-16/junghkim/model\n",
      "Epoch end..\n",
      "epoch time : 498.1607726860093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [08:27<2:40:38, 507.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch start..\n",
      "Epoch [2/20], Step [3/237], Loss: 0.0000002109, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [6/237], Loss: 0.0000001497, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [9/237], Loss: 0.0000006430, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [12/237], Loss: 0.0000001004, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [15/237], Loss: 0.0000001494, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [18/237], Loss: 0.0000001173, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [21/237], Loss: 0.0000001867, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [24/237], Loss: 0.0000000895, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [27/237], Loss: 0.0000002067, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [30/237], Loss: 0.0000001305, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [33/237], Loss: 0.0000001773, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [36/237], Loss: 0.0000003816, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [39/237], Loss: 0.0000001063, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [42/237], Loss: 0.0000010134, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [45/237], Loss: 0.0000001530, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [48/237], Loss: 0.0000000345, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [51/237], Loss: 0.0000000823, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [54/237], Loss: 0.0000001449, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [57/237], Loss: 0.0000001281, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [60/237], Loss: 0.0000000671, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [63/237], Loss: 0.0000004636, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [66/237], Loss: 0.0000001177, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [69/237], Loss: 0.0000001466, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [72/237], Loss: 0.0000000870, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [75/237], Loss: 0.0000002234, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [78/237], Loss: 0.0000001629, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [81/237], Loss: 0.0000007555, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [84/237], Loss: 0.0000001647, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [87/237], Loss: 0.0000001349, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [90/237], Loss: 0.0000002162, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [93/237], Loss: 0.0000000946, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [96/237], Loss: 0.0000000855, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [99/237], Loss: 0.0000040359, Accuracy: 98.44%, F1_Score: 0.98\n",
      "Epoch [2/20], Step [102/237], Loss: 0.0000001962, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [105/237], Loss: 0.0000000486, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [108/237], Loss: 0.0000003956, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [111/237], Loss: 0.0000003821, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [114/237], Loss: 0.0000012603, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [117/237], Loss: 0.0000002863, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [120/237], Loss: 0.0000003938, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [123/237], Loss: 0.0000000726, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [126/237], Loss: 0.0000001770, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [129/237], Loss: 0.0000004665, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [132/237], Loss: 0.0000001413, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [135/237], Loss: 0.0000002234, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [138/237], Loss: 0.0000000686, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [141/237], Loss: 0.0000001320, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [144/237], Loss: 0.0000001854, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [147/237], Loss: 0.0000005355, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [150/237], Loss: 0.0000002705, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [153/237], Loss: 0.0000003107, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [156/237], Loss: 0.0000000088, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [159/237], Loss: 0.0000009847, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [162/237], Loss: 0.0000000899, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [165/237], Loss: 0.0000000880, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [168/237], Loss: 0.0000001152, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [171/237], Loss: 0.0000002694, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [174/237], Loss: 0.0000000717, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [177/237], Loss: 0.0000002372, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [180/237], Loss: 0.0000000560, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [183/237], Loss: 0.0000000905, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [186/237], Loss: 0.0000000966, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [189/237], Loss: 0.0000000406, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [192/237], Loss: 0.0000001972, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [195/237], Loss: 0.0000003634, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [198/237], Loss: 0.0000001246, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [201/237], Loss: 0.0000000354, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [204/237], Loss: 0.0000000701, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [207/237], Loss: 0.0000003925, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [210/237], Loss: 0.0000001566, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [213/237], Loss: 0.0000001264, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [216/237], Loss: 0.0000000619, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [219/237], Loss: 0.0000001288, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [222/237], Loss: 0.0000001499, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [225/237], Loss: 0.0000001931, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [228/237], Loss: 0.0000000251, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [231/237], Loss: 0.0000001504, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [234/237], Loss: 0.0000001404, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [2/20], Step [237/237], Loss: 0.0000029718, Accuracy: 100.00%, F1_Score: 1.00\n",
      "------------Epoch Finish------------\n",
      "Epoch [2/20], Accuracy: 99.99%, F1_Score: 1.00\n",
      "Start validation #2\n",
      "Validation #2  Accuracy: 100.00% F1_Score: 1.00 Average Loss: 0.0000001574\n",
      "avrg loss : 0.0000001574\n",
      "Best performance at epoch: 2\n",
      "Save model in /opt/ml/level1-image-classification-level1-recsys-16/junghkim/model\n",
      "Epoch end..\n",
      "epoch time : 503.70993564499076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [17:00<2:32:41, 508.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch start..\n",
      "Epoch [3/20], Step [3/237], Loss: 0.0000000949, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [6/237], Loss: 0.0000000988, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [9/237], Loss: 0.0000000170, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [12/237], Loss: 0.0000005778, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [15/237], Loss: 0.0000000594, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [18/237], Loss: 0.0000002041, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [21/237], Loss: 0.0000001004, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [24/237], Loss: 0.0000003369, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [27/237], Loss: 0.0000000144, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [30/237], Loss: 0.0000000560, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [33/237], Loss: 0.0000002133, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [36/237], Loss: 0.0000001699, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [39/237], Loss: 0.0000001554, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [42/237], Loss: 0.0000002240, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [45/237], Loss: 0.0000000798, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [48/237], Loss: 0.0000003708, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [51/237], Loss: 0.0000002160, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [54/237], Loss: 0.0000002112, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [57/237], Loss: 0.0000001445, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [60/237], Loss: 0.0000000848, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [63/237], Loss: 0.0000001534, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [66/237], Loss: 0.0000001014, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [69/237], Loss: 0.0000001282, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [72/237], Loss: 0.0000000309, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [75/237], Loss: 0.0000001474, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [78/237], Loss: 0.0000000975, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [81/237], Loss: 0.0000001461, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [84/237], Loss: 0.0000001634, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [87/237], Loss: 0.0000004186, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [90/237], Loss: 0.0000000503, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [93/237], Loss: 0.0000001029, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [96/237], Loss: 0.0000001255, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [99/237], Loss: 0.0000000379, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [102/237], Loss: 0.0000000208, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [105/237], Loss: 0.0000000696, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [108/237], Loss: 0.0000000559, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [111/237], Loss: 0.0000001327, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [114/237], Loss: 0.0000001257, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [117/237], Loss: 0.0000001182, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [120/237], Loss: 0.0000000718, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [123/237], Loss: 0.0000000402, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [126/237], Loss: 0.0000001396, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [129/237], Loss: 0.0000003763, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [132/237], Loss: 0.0000001773, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [135/237], Loss: 0.0000000790, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [138/237], Loss: 0.0000000660, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [141/237], Loss: 0.0000001710, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [144/237], Loss: 0.0000001806, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [147/237], Loss: 0.0000001592, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [150/237], Loss: 0.0000000605, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [153/237], Loss: 0.0000001153, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [156/237], Loss: 0.0000000146, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [159/237], Loss: 0.0000000612, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [162/237], Loss: 0.0000000506, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [165/237], Loss: 0.0000000694, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [168/237], Loss: 0.0000000380, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [171/237], Loss: 0.0000004273, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [174/237], Loss: 0.0000001991, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [177/237], Loss: 0.0000002077, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [180/237], Loss: 0.0000000412, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [183/237], Loss: 0.0000000250, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [186/237], Loss: 0.0000000439, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [189/237], Loss: 0.0000000822, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [192/237], Loss: 0.0000001798, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [195/237], Loss: 0.0000000487, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [198/237], Loss: 0.0000001089, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [201/237], Loss: 0.0000000738, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [204/237], Loss: 0.0000000768, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [207/237], Loss: 0.0000000779, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [210/237], Loss: 0.0000001816, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [213/237], Loss: 0.0000001370, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [216/237], Loss: 0.0000002478, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [219/237], Loss: 0.0000001932, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [222/237], Loss: 0.0000000946, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [225/237], Loss: 0.0000000883, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [228/237], Loss: 0.0000001804, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [231/237], Loss: 0.0000000167, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [234/237], Loss: 0.0000000636, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [3/20], Step [237/237], Loss: 0.0000000454, Accuracy: 100.00%, F1_Score: 1.00\n",
      "------------Epoch Finish------------\n",
      "Epoch [3/20], Accuracy: 100.00%, F1_Score: 1.00\n",
      "Start validation #3\n",
      "Validation #3  Accuracy: 100.00% F1_Score: 1.00 Average Loss: 0.0000001102\n",
      "avrg loss : 0.0000001102\n",
      "Best performance at epoch: 3\n",
      "Save model in /opt/ml/level1-image-classification-level1-recsys-16/junghkim/model\n",
      "Epoch end..\n",
      "epoch time : 504.5886475419975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [25:33<2:24:36, 510.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch start..\n",
      "Epoch [4/20], Step [3/237], Loss: 0.0000002255, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [6/237], Loss: 0.0000001545, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [9/237], Loss: 0.0000000434, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [12/237], Loss: 0.0000001550, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [15/237], Loss: 0.0000000125, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [18/237], Loss: 0.0000000820, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [21/237], Loss: 0.0000000236, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [24/237], Loss: 0.0000002747, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [27/237], Loss: 0.0000003278, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [30/237], Loss: 0.0000000791, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [33/237], Loss: 0.0000000596, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [36/237], Loss: 0.0000000546, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [39/237], Loss: 0.0000000480, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [42/237], Loss: 0.0000001576, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [45/237], Loss: 0.0000000691, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [48/237], Loss: 0.0000000881, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [51/237], Loss: 0.0000001319, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [54/237], Loss: 0.0000001174, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [57/237], Loss: 0.0000001555, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [60/237], Loss: 0.0000000535, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [63/237], Loss: 0.0000000430, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [66/237], Loss: 0.0000000493, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [69/237], Loss: 0.0000002989, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [72/237], Loss: 0.0000000640, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [75/237], Loss: 0.0000000473, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [78/237], Loss: 0.0000000800, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [81/237], Loss: 0.0000000398, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [84/237], Loss: 0.0000000553, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [87/237], Loss: 0.0000000296, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [90/237], Loss: 0.0000000441, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [93/237], Loss: 0.0000000269, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [96/237], Loss: 0.0000000405, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [99/237], Loss: 0.0000000431, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [102/237], Loss: 0.0000000408, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [105/237], Loss: 0.0000000427, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [108/237], Loss: 0.0000001200, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [111/237], Loss: 0.0000000238, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [114/237], Loss: 0.0000000422, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [117/237], Loss: 0.0000001494, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [120/237], Loss: 0.0000000817, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [123/237], Loss: 0.0000001009, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [126/237], Loss: 0.0000000771, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [129/237], Loss: 0.0000000511, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [132/237], Loss: 0.0000000271, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [135/237], Loss: 0.0000001711, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [138/237], Loss: 0.0000000603, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [141/237], Loss: 0.0000002596, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [144/237], Loss: 0.0000000397, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [147/237], Loss: 0.0000000256, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [150/237], Loss: 0.0000004283, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [153/237], Loss: 0.0000000662, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [156/237], Loss: 0.0000000344, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [159/237], Loss: 0.0000000815, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [162/237], Loss: 0.0000000604, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [165/237], Loss: 0.0000000412, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [168/237], Loss: 0.0000002489, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [171/237], Loss: 0.0000000405, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [174/237], Loss: 0.0000000896, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [177/237], Loss: 0.0000000731, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [180/237], Loss: 0.0000000321, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [183/237], Loss: 0.0000001227, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [186/237], Loss: 0.0000000552, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [189/237], Loss: 0.0000001947, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [192/237], Loss: 0.0000001025, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [195/237], Loss: 0.0000000163, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [198/237], Loss: 0.0000000836, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [201/237], Loss: 0.0000000423, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [204/237], Loss: 0.0000000643, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [207/237], Loss: 0.0000000156, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [210/237], Loss: 0.0000000835, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [213/237], Loss: 0.0000000459, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [216/237], Loss: 0.0000001397, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [219/237], Loss: 0.0000001737, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [222/237], Loss: 0.0000000299, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [225/237], Loss: 0.0000001775, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [228/237], Loss: 0.0000000916, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [231/237], Loss: 0.0000001006, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [234/237], Loss: 0.0000001107, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [4/20], Step [237/237], Loss: 0.0000001017, Accuracy: 100.00%, F1_Score: 1.00\n",
      "------------Epoch Finish------------\n",
      "Epoch [4/20], Accuracy: 100.00%, F1_Score: 1.00\n",
      "Start validation #4\n",
      "Validation #4  Accuracy: 100.00% F1_Score: 1.00 Average Loss: 0.0000000847\n",
      "avrg loss : 0.0000000847\n",
      "Best performance at epoch: 4\n",
      "Save model in /opt/ml/level1-image-classification-level1-recsys-16/junghkim/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [33:57<2:15:32, 508.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch end..\n",
      "epoch time : 503.49104819097556\n",
      "Epoch start..\n",
      "Epoch [5/20], Step [3/237], Loss: 0.0000000525, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [6/237], Loss: 0.0000001929, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [9/237], Loss: 0.0000001636, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [12/237], Loss: 0.0000000192, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [15/237], Loss: 0.0000000186, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [18/237], Loss: 0.0000000101, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [21/237], Loss: 0.0000000379, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [24/237], Loss: 0.0000000309, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [27/237], Loss: 0.0000000798, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [30/237], Loss: 0.0000000504, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [33/237], Loss: 0.0000000434, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [36/237], Loss: 0.0000002746, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [39/237], Loss: 0.0000000320, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [42/237], Loss: 0.0000000836, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [45/237], Loss: 0.0000000122, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [48/237], Loss: 0.0000000962, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [51/237], Loss: 0.0000000465, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [54/237], Loss: 0.0000000274, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [57/237], Loss: 0.0000000265, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [60/237], Loss: 0.0000000488, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [63/237], Loss: 0.0000001471, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [66/237], Loss: 0.0000001932, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [69/237], Loss: 0.0000000587, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [72/237], Loss: 0.0000000796, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [75/237], Loss: 0.0000000557, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [78/237], Loss: 0.0000000434, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [81/237], Loss: 0.0000000384, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [84/237], Loss: 0.0000000572, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [87/237], Loss: 0.0000001971, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [90/237], Loss: 0.0000000266, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [93/237], Loss: 0.0000001105, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [96/237], Loss: 0.0000000414, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [99/237], Loss: 0.0000000544, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [102/237], Loss: 0.0000002503, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [105/237], Loss: 0.0000000445, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [108/237], Loss: 0.0000001290, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [111/237], Loss: 0.0000001930, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [114/237], Loss: 0.0000000839, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [117/237], Loss: 0.0000000974, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [120/237], Loss: 0.0000001355, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [123/237], Loss: 0.0000000379, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [126/237], Loss: 0.0000000869, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [129/237], Loss: 0.0000000559, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [132/237], Loss: 0.0000001154, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [135/237], Loss: 0.0000000850, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [138/237], Loss: 0.0000001724, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [141/237], Loss: 0.0000001018, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [144/237], Loss: 0.0000000293, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [147/237], Loss: 0.0000000317, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [150/237], Loss: 0.0000001516, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [153/237], Loss: 0.0000001709, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [156/237], Loss: 0.0000000369, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [159/237], Loss: 0.0000000291, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [162/237], Loss: 0.0000000306, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [165/237], Loss: 0.0000000489, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [168/237], Loss: 0.0000000450, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [171/237], Loss: 0.0000000241, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [174/237], Loss: 0.0000000560, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [177/237], Loss: 0.0000000408, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [180/237], Loss: 0.0000000199, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [183/237], Loss: 0.0000000179, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [186/237], Loss: 0.0000000998, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [189/237], Loss: 0.0000000418, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [192/237], Loss: 0.0000000130, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [195/237], Loss: 0.0000000101, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [198/237], Loss: 0.0000000190, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [201/237], Loss: 0.0000000778, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [204/237], Loss: 0.0000000465, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [207/237], Loss: 0.0000000510, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [210/237], Loss: 0.0000001439, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [213/237], Loss: 0.0000000388, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [216/237], Loss: 0.0000000329, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [219/237], Loss: 0.0000001312, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [222/237], Loss: 0.0000000349, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [225/237], Loss: 0.0000000305, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [228/237], Loss: 0.0000000350, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [231/237], Loss: 0.0000000279, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [234/237], Loss: 0.0000000377, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [5/20], Step [237/237], Loss: 0.0000000440, Accuracy: 100.00%, F1_Score: 1.00\n",
      "------------Epoch Finish------------\n",
      "Epoch [5/20], Accuracy: 100.00%, F1_Score: 1.00\n",
      "Start validation #5\n",
      "Validation #5  Accuracy: 100.00% F1_Score: 1.00 Average Loss: 0.0000000725\n",
      "avrg loss : 0.0000000725\n",
      "Best performance at epoch: 5\n",
      "Save model in /opt/ml/level1-image-classification-level1-recsys-16/junghkim/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [42:20<2:06:43, 506.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch end..\n",
      "epoch time : 503.65809209801955\n",
      "Epoch start..\n",
      "Epoch [6/20], Step [3/237], Loss: 0.0000000645, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [6/20], Step [6/237], Loss: 0.0000000291, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [6/20], Step [9/237], Loss: 0.0000000281, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [6/20], Step [12/237], Loss: 0.0000000207, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [6/20], Step [15/237], Loss: 0.0000000340, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [6/20], Step [18/237], Loss: 0.0000000628, Accuracy: 100.00%, F1_Score: 1.00\n",
      "Epoch [6/20], Step [21/237], Loss: 0.0000000936, Accuracy: 100.00%, F1_Score: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [43:03<2:09:10, 516.72s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-d8a8845fc2f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-c9dbe920d1ef>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs, model, data_loader, criterion, optimizer, saved_dir, val_every, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(num_epochs, model, train_dataloader, criterion, optimizer, saved_dir, val_every, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd49c163-a9af-4951-815e-1eb81f6f7422",
   "metadata": {},
   "source": [
    "# 모델 불러오는 곳\n",
    "### 어제 돌리다가 일어나서 돌리던 모델 다시 돌리고 싶으면. 아래에서 모델 불러와서 쓰면 됩니다.\n",
    "### 대신 완전히 껐다가 킨거면, 위에 train 함수 전까지 다 실행시키고 아래 코드 실행하고 다시 train 학습 하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23649c9f-dc9d-4799-b28a-1e743ed32b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = '/opt/ml/level1-image-classification-level1-recsys-16/junghkim/model/vit_focal_maskOX.pt'\n",
    "checkpoint = torch.load(model_path,map_location=device)\n",
    "state_dict = checkpoint['net']\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc340cc-60cc-43d1-b88b-4459a1ebc652",
   "metadata": {},
   "source": [
    "# 틀린 부분 확인하는 곳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "905ca92f-f28a-4b20-8435-dee1ddcac0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_eval(raw_data, dataloader, model, device):\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, (X,y) in enumerate(dataloader):\n",
    "            model_pred = model.forward(X.to(device))\n",
    "            _, y_pred = torch.max(model_pred, 1)\n",
    "            \n",
    "            result.append([valid.iloc[i]['path'], y_pred.cpu().numpy()[0], y.cpu().numpy()[0]])\n",
    "    result = pd.DataFrame(result, columns=['path', 'pred', 'target'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac6eed2f-09ab-4b1d-9af0-714641f2f1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_testing_dataloader = DataLoader(valid_dataset, shuffle = False)\n",
    "check_eval_df = check_eval(valid, valid_testing_dataloader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2debc17-0880-4d98-8524-1d4830eaa40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_df = check_eval_df[check_eval_df['pred'] != check_eval_df['target']]\n",
    "wrong_df = wrong_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b9c5666-92c8-48d5-b3a4-47558c88b99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_(df):\n",
    "    plt.figure(figsize = (50, 100))\n",
    "    row = 7\n",
    "    for i in range(df.shape[0]):\n",
    "        plt.subplot(row +1, df.shape[0]//row, i+1)\n",
    "        plt.imshow(Image.open(df['path'][i]))\n",
    "        plt.title(f'target:{df[\"target\"][i]} . pred:{df[\"pred\"][i]}', color = 'r', size=20)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e9e16c9-7448-47d5-9ad0-10f7ae01fbde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 3600x7200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_(wrong_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7114c61c-c55b-4552-847a-b7d33e3397bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_df.to_csv('/opt/ml/vit_focal_maskOX.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b95e03-2c3c-4763-98fb-0bde41e50c60",
   "metadata": {},
   "source": [
    "# 아래는 실제 제출하기 위한 공간입니다\n",
    "## class(18)가 아니면 안쓰면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa13bb8e-8783-4b73-a426-979ce5eec38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a4971d94-cf69-45c6-8e54-7fc92c04492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '/opt/ml/input/data/eval/'\n",
    "\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "\n",
    "test_data = TestDataset(image_paths, transform=transform['test'])\n",
    "test_loader = DataLoader(test_data, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8ebf2b12-1460-41ad-8069-bbeca818bb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([13], device='cuda:0')\n",
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "model.eval()\n",
    "i = 0\n",
    "for images in test_loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        _, predict = torch.max(pred, 1)\n",
    "        if i < 3:\n",
    "            i+=1\n",
    "            print(predict)\n",
    "        all_predictions.extend(predict.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submit_dir = '/opt/ml/level1-image-classification-level1-recsys-16/junghkim/submit'\n",
    "submission.to_csv(os.path.join(submit_dir, 'vit_focal.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5770cda-3fbe-4494-beb5-579442077583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
